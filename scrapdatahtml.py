# -*- coding: utf-8 -*-
"""ScrapDataHTML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nuqw7ESQKahBQfz2rs2d-uCCZ0jwSUp0
"""

import requests
from bs4 import BeautifulSoup
import csv

lista = []
url_list = []

url_list = [
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/32'
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/31',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/29',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/28',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/27',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/26',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/25',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/24',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/23',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/22',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/19',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/18',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/17',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/16',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/13',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/12',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/11',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/10',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/8',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/9',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/7',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/6',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/4',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/3',
  'https://eademfoco.cecierj.edu.br/index.php/Revista/issue/view/1'
    ]

for link in url_list:
  response = requests.get(link)
  soup = BeautifulSoup(response.content, 'html.parser')

  autores = soup.find_all('div', class_='authors')
  for i in autores:
    lista.append(i.get_text())

for i in range(len(lista)):
  lista[i] = lista[i].replace('\t', '').replace('\n', '')

print(len(lista))

nodos = []
with open('./nodes.csv', 'w') as csvfile:
  for k in range(0, len(lista)):
    nodos = lista[k].split(",")
    for s in range(0, len(nodos)):
        csv.writer(csvfile, delimiter=',').writerow([nodos[s].strip()])

edges = []
with open('./edges.csv', 'w') as csvfile:
  for m in range(0, len(lista)):
    edges = lista[m].split(",")
    for n in range(0, len(edges)):
          csv.writer(csvfile, delimiter=',').writerow([edges[n-1].strip(),edges[n].strip()])
